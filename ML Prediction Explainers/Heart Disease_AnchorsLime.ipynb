{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separate-amber",
   "metadata": {},
   "source": [
    "# UCI Heart Disease Detection\n",
    "\n",
    "This notebook is used as part of my thesis, comparing different XAI methods and libraries.\n",
    "<br/>\n",
    "The purpose of the created models is to classify if a patient is either healthy or has a heart disease.\n",
    "<br/>\n",
    "<br/>\n",
    "Attributes:\n",
    "1. age\n",
    "2. sex (1=male, 0=female)\n",
    "3. chest pain type (4 values)\n",
    "4. resting blood pressure\n",
    "5. serum cholesterol in milligrams per deciliter (mg/dl)\n",
    "6. fasting blood sugar > 120 mg/dl\n",
    "7. resting electrocardiographic results (values 0,1,2)\n",
    "8. maximum heart rate achieved\n",
    "9. exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. the slope of the peak exercise ST segment\n",
    "12. number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-sequence",
   "metadata": {},
   "source": [
    "## 1 Set up Environment and Dataset <a class=\"anchor\" id=\"ch1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-swimming",
   "metadata": {},
   "source": [
    "### 1.1 Load Libraries and Set Up Parameters <a class=\"anchor\" id=\"ch1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for reproduction\n",
    "seedNum = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "import catboost\n",
    "import shap\n",
    "import lime\n",
    "import graphviz\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from alibi.explainers import AnchorTabular, CounterFactualProto, CounterFactual\n",
    "from alibi.utils.mapping import ohe_to_ord, ord_to_ohe\n",
    "from datetime import datetime\n",
    "\n",
    "# required installs:\n",
    "# pip install shap\n",
    "# pip install lime\n",
    "# pip install alibi\n",
    "# conda install python-graphviz AND install from https://graphviz.org/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# set up n_jobs\n",
    "n_jobs = 6\n",
    "\n",
    "# set flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.20\n",
    "\n",
    "# set number of folds for cross validation\n",
    "n_folds = 10\n",
    "\n",
    "# set various default modeling parameters\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of columns for explanation compatability\n",
    "new_order=[\"sex\",\"cp\",\"fbs\",\"restecg\",\"exang\",\"slope\",\"thal\",\"age\",\"trestbps\",\"chol\",\"thalach\",\"oldpeak\",\"ca\",\"target\"]\n",
    "\n",
    "# dictionary of categorical variable values\n",
    "category_map={0: [\"female\", \"male\"],\n",
    "              1: [\"typical angina\",\"atypical angina\",\"non-anginal pain\",\"asymptomatic\"],\n",
    "              2: [\"below 120 mg/dl\",\"above 120 ml/dl\"],\n",
    "              3: [\"normal\",\"ST-T wave abnormality\",\"probable left ventricular hypertrophy\"],\n",
    "              4: [\"no\",\"yes\"],\n",
    "              5: [\"upsloping\",\"flat\",\"downsloping\"],\n",
    "              6: [\"no info\",\"normal\",\"fixed defect\",\"reversable defect\"]\n",
    "             }\n",
    "\n",
    "# dict of column names for renaming\n",
    "col_names = {\"cp\":'chest pain type', \"trestbps\":'resting blood pressure', \"chol\":'serum cholesterol (mg/dl)',\n",
    "             \"fbs\":'fasting blood sugar', \"restecg\":'resting ecg results',\n",
    "             \"thalach\":'maximum heart rate achieved', \"exang\":'exercise induced angina',\n",
    "             \"oldpeak\":'exercise induced ST depression',\n",
    "             \"slope\":'slope of peak exercise ST segment', \"ca\":'vessels colored by flourosopy', \"thal\":\"thalassemia type\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset_path = 'data/heart.csv'\n",
    "Xy_original = pd.read_csv(dataset_path)\n",
    "Xy_original = Xy_original[new_order]\n",
    "Xy_original.rename(columns=col_names, inplace=True)\n",
    "Xy_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-syndrome",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing <a class=\"anchor\" id=\"ch1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(Xy_original.columns)\n",
    "totAttr = totCol-1\n",
    "\n",
    "X_original = Xy_original.iloc[:,0:totAttr]\n",
    "y_original = Xy_original.iloc[:,totAttr]\n",
    "\n",
    "print(\"Xy_original.shape: {} X_original.shape: {} y_original.shape: {}\".format(Xy_original.shape, X_original.shape, y_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data further into training and test datasets\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_original, y_original, test_size=splitPercentage, \n",
    "                                                                stratify=y_original, random_state=seedNum)\n",
    "\n",
    "print(\"X_train.shape: {} y_train_df.shape: {}\".format(X_train_df.shape, y_train_df.shape))\n",
    "print(\"X_test_df.shape: {} y_test_df.shape: {}\".format(X_test_df.shape, y_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the training and testing datasets for the modeling activities\n",
    "X_train = X_train_df.to_numpy()\n",
    "y_train = y_train_df.to_numpy()\n",
    "X_test = X_test_df.to_numpy()\n",
    "y_test = y_test_df.to_numpy()\n",
    "print(\"X_train.shape: {} y_train.shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"X_test.shape: {} y_test.shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor\n",
    "\n",
    "feature_names = X_original.columns.values\n",
    "\n",
    "num_features = [x for x in range(len(feature_names)) if x not in list(category_map.keys())]\n",
    "num_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "categorical_features = list(category_map.keys())\n",
    "categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "                                          ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('num', num_transformer, num_features),\n",
    "                                               ('cat', categorical_transformer, categorical_features)])\n",
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-engagement",
   "metadata": {},
   "source": [
    "## 2 Tree-based Modeling <a class=\"anchor\" id=\"ch2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-alliance",
   "metadata": {},
   "source": [
    "### 2.1 Try Some Untuned  Models <a class=\"anchor\" id=\"ch2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Algorithms Spot-Checking Array\n",
    "\n",
    "startTimeModule = datetime.now()\n",
    "train_models = []\n",
    "train_results = []\n",
    "train_model_names = []\n",
    "train_metrics = []\n",
    "train_models.append(('LR', LogisticRegression(random_state=seedNum, n_jobs=n_jobs)))\n",
    "train_models.append(('KN', KNeighborsClassifier(n_jobs=n_jobs)))\n",
    "train_models.append(('DT', DecisionTreeClassifier(random_state=seedNum)))\n",
    "train_models.append(('BT', BaggingClassifier(random_state=seedNum, n_jobs=n_jobs)))\n",
    "train_models.append(('RF', RandomForestClassifier(random_state=seedNum, n_jobs=n_jobs)))\n",
    "train_models.append(('ET', ExtraTreesClassifier(random_state=seedNum, n_jobs=n_jobs)))\n",
    "train_models.append(('GB', GradientBoostingClassifier(random_state=seedNum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate models in turn\n",
    "\n",
    "for name, model in train_models:\n",
    "    startTimeModule = datetime.now()\n",
    "    kfold = KFold(n_splits=n_folds)\n",
    "    cv_results = cross_val_score(model, preprocessor.transform(X_train), y_train, cv=kfold, scoring=scoring)\n",
    "    train_results.append(cv_results)\n",
    "    train_model_names.append(name)\n",
    "    train_metrics.append(cv_results.mean())\n",
    "    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))\n",
    "    print(model)\n",
    "    print ('Model training time:', (datetime.now() - startTimeModule), '\\n')\n",
    "print ('Average metrics ('+scoring+') from all models:',np.mean(train_metrics))\n",
    "print ('Total training time for all models:',(datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-mission",
   "metadata": {},
   "source": [
    "### 2.2 Train and Set Up Reference Models <a class=\"anchor\" id=\"ch2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-appendix",
   "metadata": {},
   "source": [
    "Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(random_state=seedNum)\n",
    "param_grid_tree = {\n",
    "    \"max_depth\": [4,6,8],\n",
    "    \"criterion\" : [\"gini\",\"entropy\"],\n",
    "    \"min_samples_leaf\": [6,10,14]\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=n_folds)\n",
    "grid = GridSearchCV(decisiontree, param_grid_tree, scoring=scoring, cv=kfold, n_jobs=n_jobs, refit=\"Recall\")\n",
    "\n",
    "grid_result = grid.fit(preprocessor.transform(X_train), y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_dt_be = grid_result.best_estimator_\n",
    "clf_dt = clf_dt_be.fit(preprocessor.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-cemetery",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "tune_model = RandomForestClassifier(random_state=seedNum, n_jobs=n_jobs)\n",
    "\n",
    "n_estimators = [100]\n",
    "criterion = [\"gini\",\"entropy\"]\n",
    "max_features =[None, \"sqrt\", 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "paramGrid = dict(n_estimators=n_estimators, criterion=criterion, max_features=max_features)\n",
    "\n",
    "kfold = KFold(n_splits=n_folds)\n",
    "grid = GridSearchCV(estimator=tune_model, param_grid=paramGrid, scoring=scoring, cv=kfold, refit=\"Recall\")\n",
    "grid_result = grid.fit(preprocessor.transform(X_train), y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_rf_be = grid_result.best_estimator_\n",
    "clf_rf = clf_rf_be.fit(preprocessor.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-engine",
   "metadata": {},
   "source": [
    "Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_new = True\n",
    "\n",
    "if(gridsearch_new):\n",
    "    param_grid_catB = {\n",
    "        \"depth\": [6,10],\n",
    "    }\n",
    "    \n",
    "    clf_catb = CatBoostClassifier(\n",
    "        eval_metric='Accuracy',\n",
    "    )\n",
    "    \n",
    "    startTimeModule = datetime.now()\n",
    "    grid_search = clf_catb.grid_search(param_grid_catB, preprocessor.transform(X_train), y_train, partition_random_seed=seedNum, plot=False)\n",
    "    \n",
    "    print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_catb.get_params()\n",
    "\n",
    "clf_cb_be = CatBoostClassifier(eval_metric='Accuracy', depth=6, verbose=False)\n",
    "\n",
    "startTimeModule = datetime.now()\n",
    "clf_cb = clf_cb_be.fit(preprocessor.transform(X_train), y_train, verbose=False)\n",
    "#print(\"Best: %f using %s\" % (grid_search.best_score_, grid_search.best_params_))\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-planner",
   "metadata": {},
   "source": [
    "### 2.3 Model Evaluation <a class=\"anchor\" id=\"ch2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = clf_dt.predict(preprocessor.transform(X_test))\n",
    "predictions_rf = clf_rf.predict(preprocessor.transform(X_test))\n",
    "predictions_cb = clf_cb.predict(preprocessor.transform(X_test))\n",
    "cv_dt = cross_val_score(clf_dt_be, preprocessor.transform(X_train), y_train, cv=kfold, scoring=scoring)\n",
    "cv_rf = cross_val_score(clf_rf_be, preprocessor.transform(X_train), y_train, cv=kfold, scoring=scoring)\n",
    "cv_cb = cross_val_score(clf_cb_be, preprocessor.transform(X_train), y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(clf_dt,\"\\n\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_dt))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_dt))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_dt.mean(), cv_dt.std()))\n",
    "print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "print(clf_rf,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_rf))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_rf))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_rf.mean(), cv_rf.std()))\n",
    "print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "print(clf_cb,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_cb))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_cb))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_cb.mean(), cv_cb.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_fpr, dt_tpr, dt_thresholds = roc_curve(y_true=y_test, y_score=predictions_dt, pos_label=1)\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_true=y_test, y_score=predictions_rf, pos_label=1)\n",
    "cb_fpr, cb_tpr, cb_thresholds = roc_curve(y_true=y_test, y_score=predictions_cb, pos_label=1)\n",
    "\n",
    "print(\"Decision Tree AUC: \",auc(dt_fpr, dt_tpr))\n",
    "print(\"Random Forest AUC: \",auc(rf_fpr, rf_tpr,))\n",
    "print(\"Catboost AUC: \",auc(cb_fpr, cb_tpr,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "plt.plot(dt_fpr, dt_tpr, label=\"Decision Tree\")\n",
    "plt.plot(rf_fpr, rf_tpr,\"brown\", label=\"Random Forest\")\n",
    "plt.plot(cb_fpr, cb_tpr, \"violet\", label=\"Gradient Boosting\")\n",
    "plt.plot([0,1],[0,1],'r-',label='Random Predictions')\n",
    "plt.plot([0,0,1,1],[0,1,1,1],'g-',label='Perfect Predictions')\n",
    "plt.legend()\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-finnish",
   "metadata": {},
   "source": [
    "# 3 Anchors <a class=\"anchor\" id=\"ch3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-replica",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf=clf_cb\n",
    "pred_idx = 1\n",
    "\n",
    "probabilities = clf.predict_proba(preprocessor.transform(X_test))\n",
    "print(\"Probabilities: \", probabilities[pred_idx])\n",
    "print(\"Correct class: \", y_test[pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"healthy\", \"sick\"]\n",
    "feature_names = X_original.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf_rf.predict_proba(preprocessor.transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = AnchorTabular(predict_fn, feature_names, categorical_names=category_map, seed=seedNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.fit(X_train, disc_perc=[25, 50, 75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = target_names\n",
    "\n",
    "anch_exp = explainer.explain(X_test[pred_idx], threshold=0.95)\n",
    "print('\\nANCHOR:\\n\\nIF %s' % ('\\n AND '.join(anch_exp.anchor)))\n",
    "print('THEN PREDICT: ', class_names[explainer.predictor(X_test[pred_idx].reshape(1, -1))[0]])\n",
    "print('\\nWITH PRECISION: %.2f' % anch_exp.precision)\n",
    "print('WITH COVERAGE: %.2f' % anch_exp.coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-overall",
   "metadata": {},
   "source": [
    "# 4 Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train ,feature_names = feature_names,class_names=class_names,\n",
    "                                                   categorical_features=categorical_features, \n",
    "                                                   categorical_names=category_map, kernel_width=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-sailing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = lime_explainer.explain_instance(X_test[pred_idx], predict_fn, num_features=5)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "explist=exp.as_list()\n",
    "explist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-seating",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
