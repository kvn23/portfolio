{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sharing-ozone",
   "metadata": {},
   "source": [
    "# iris Classification\n",
    "\n",
    "This notebook is used as part of my thesis, comparing different XAI methods and libraries.\n",
    "<br/>\n",
    "The purpose of the created models is to classify if a sample belongs to one of three iris species.\n",
    "<br/>\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-heritage",
   "metadata": {},
   "source": [
    "## 1 Set up Environment and Dataset <a class=\"anchor\" id=\"ch1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-tolerance",
   "metadata": {},
   "source": [
    "### 1.1 Load Libraries and Set Up Parameters <a class=\"anchor\" id=\"ch1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "inclusive-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for reproduction\n",
    "seedNum = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "national-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "import catboost\n",
    "import shap\n",
    "import lime\n",
    "import graphviz\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from alibi.explainers import AnchorTabular, CounterFactualProto, CounterFactual\n",
    "from datetime import datetime\n",
    "\n",
    "# required installs:\n",
    "# pip install shap\n",
    "# pip install lime\n",
    "# pip install alibi\n",
    "# conda install python-graphviz AND install from https://graphviz.org/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "checked-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# set up n_jobs\n",
    "n_jobs = 6\n",
    "\n",
    "# set flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.20\n",
    "\n",
    "# set number of folds for cross validation\n",
    "n_folds = 10\n",
    "\n",
    "# set various default modeling parameters\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ambient-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = {\"SepalLengthCm\":'Sepal Length (cm)', \"SepalWidthCm\":'Sepal Width (cm)',\n",
    "             \"PetalLengthCm\":'Petal Length (cm)', \"PetalWidthCm\":'Petal Width (cm)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decent-graham",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/iris.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7fa694fb1a6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdataset_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'data/iris.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mXy_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mXy_original\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mXy_original\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/iris.csv'"
     ]
    }
   ],
   "source": [
    "#import dataset\n",
    "dataset_path = 'data/iris.csv'\n",
    "Xy_original = pd.read_csv(dataset_path)\n",
    "Xy_original.rename(columns=col_names, inplace=True)\n",
    "Xy_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(Xy_original.columns)\n",
    "totAttr = totCol-1\n",
    "\n",
    "\n",
    "X_original = Xy_original.iloc[:,0:totAttr]\n",
    "y_original = Xy_original.iloc[:,totAttr]\n",
    "\n",
    "print(\"Xy_original.shape: {} X_original.shape: {} y_original.shape: {}\".format(Xy_original.shape, X_original.shape, y_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original = X_original.drop(\"Id\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-twist",
   "metadata": {},
   "source": [
    "### 1.2 Quick EDA <a class=\"anchor\" id=\"ch1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of labels\n",
    "Xy_original.groupby('Species').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_counts= Xy_original['Species'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "fault_counts_barplot = sns.barplot(x = fault_counts.index,y = fault_counts.values, ax = ax[0])\n",
    "fault_counts_barplot.set_ylabel('Number of classes in the dataset')\n",
    "fault_counts.plot.pie(autopct=\"%1.1f%%\", ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-falls",
   "metadata": {},
   "source": [
    "### 1.3 Data Cleaning and Preparation <a class=\"anchor\" id=\"ch1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-conspiracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data further into training and test datasets\n",
    "if (splitDataset):\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_original, y_original, test_size=splitPercentage, \n",
    "                                                                    stratify=y_original, random_state=seedNum)\n",
    "else:\n",
    "    X_train_df, y_train_df = X_original, y_original\n",
    "    X_test_df, y_test_df = X_original, y_original\n",
    "print(\"X_train.shape: {} y_train_df.shape: {}\".format(X_train_df.shape, y_train_df.shape))\n",
    "print(\"X_test_df.shape: {} y_test_df.shape: {}\".format(X_test_df.shape, y_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the training and testing datasets for the modeling activities\n",
    "X_train = X_train_df.to_numpy()\n",
    "y_train = y_train_df.to_numpy()\n",
    "X_test = X_test_df.to_numpy()\n",
    "y_test = y_test_df.to_numpy()\n",
    "print(\"X_train.shape: {} y_train.shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"X_test.shape: {} y_test.shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-timer",
   "metadata": {},
   "source": [
    "## 2 Tree-based Modeling <a class=\"anchor\" id=\"ch2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-newman",
   "metadata": {},
   "source": [
    "### 2.1 Try Some Untuned  Models <a class=\"anchor\" id=\"ch2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Algorithms Spot-Checking Array\n",
    "\n",
    "startTimeModule = datetime.now()\n",
    "train_models = []\n",
    "train_results = []\n",
    "train_model_names = []\n",
    "train_metrics = []\n",
    "train_models.append(('DT', DecisionTreeClassifier(random_state=seedNum)))\n",
    "train_models.append(('BT', BaggingClassifier(random_state=seedNum, n_jobs=n_jobs)))\n",
    "train_models.append(('RF', RandomForestClassifier(random_state=seedNum, n_jobs=n_jobs)))\n",
    "train_models.append(('ET', ExtraTreesClassifier(random_state=seedNum, n_jobs=n_jobs)))\n",
    "train_models.append(('GB', GradientBoostingClassifier(random_state=seedNum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate models in turn\n",
    "\n",
    "for name, model in train_models:\n",
    "    startTimeModule = datetime.now()\n",
    "    kfold = KFold(n_splits=n_folds)\n",
    "    cv_results = cross_val_score(model, X_original, y_original, cv=kfold, scoring=scoring)\n",
    "    train_results.append(cv_results)\n",
    "    train_model_names.append(name)\n",
    "    train_metrics.append(cv_results.mean())\n",
    "    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))\n",
    "    print(model)\n",
    "    print ('Model training time:', (datetime.now() - startTimeModule), '\\n')\n",
    "print ('Average metrics ('+scoring+') from all models:',np.mean(train_metrics))\n",
    "print ('Total training time for all models:',(datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-danger",
   "metadata": {},
   "source": [
    "### 2.2 Train and Set Up Reference Models <a class=\"anchor\" id=\"ch2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-group",
   "metadata": {},
   "source": [
    "Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(random_state=seedNum)\n",
    "param_grid_tree = {\n",
    "    \"max_depth\": [6,8],\n",
    "    \"criterion\" : [\"gini\",\"entropy\"],\n",
    "    \"min_samples_leaf\": [6,10,14]\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=n_folds)\n",
    "grid = GridSearchCV(decisiontree, param_grid_tree, scoring=scoring, cv=kfold, n_jobs=n_jobs, refit=\"Recall\")\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_dt_be = grid_result.best_estimator_\n",
    "clf_dt = clf_dt_be.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-slave",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "tune_model = RandomForestClassifier(random_state=seedNum, n_jobs=n_jobs)\n",
    "\n",
    "n_estimators = [100]\n",
    "criterion = [\"gini\",\"entropy\"]\n",
    "max_features =[None, \"sqrt\", 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "paramGrid = dict(n_estimators=n_estimators, criterion=criterion, max_features=max_features)\n",
    "\n",
    "kfold = KFold(n_splits=n_folds)\n",
    "grid = GridSearchCV(estimator=tune_model, param_grid=paramGrid, scoring=scoring, cv=kfold, refit=\"Accuracy\")\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_rf_be = grid_result.best_estimator_\n",
    "clf_rf = clf_rf_be.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-wiring",
   "metadata": {},
   "source": [
    "Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_new = True\n",
    "\n",
    "if(gridsearch_new):\n",
    "    param_grid_catB = {\n",
    "        \"depth\": [6,10],\n",
    "    }\n",
    "    \n",
    "    clf_catb = CatBoostClassifier(\n",
    "        eval_metric='Accuracy',\n",
    "    )\n",
    "    \n",
    "    startTimeModule = datetime.now()\n",
    "    grid_search = clf_catb.grid_search(param_grid_catB, X_train, y_train, partition_random_seed=seedNum, plot=False)\n",
    "    \n",
    "    print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_catb.get_params()\n",
    "\n",
    "clf_cb_be = CatBoostClassifier(eval_metric='Accuracy', depth=6, verbose=False)\n",
    "clf_cb = clf_cb_be.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-ridge",
   "metadata": {},
   "source": [
    "### 2.3 Model Evaluation <a class=\"anchor\" id=\"ch2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = clf_dt.predict(X_test)\n",
    "predictions_rf = clf_rf.predict(X_test)\n",
    "predictions_cb = clf_cb.predict(X_test)\n",
    "cv_dt = cross_val_score(clf_dt_be, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "cv_rf = cross_val_score(clf_rf_be, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "cv_cb = cross_val_score(clf_cb_be, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(clf_dt,\"\\n\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_dt))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_dt))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_dt.mean(), cv_dt.std()))\n",
    "print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "print(clf_rf,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_rf))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_rf))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_rf.mean(), cv_rf.std()))\n",
    "print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "print(clf_cb,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_cb))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_cb))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_cb.mean(), cv_cb.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-shannon",
   "metadata": {},
   "source": [
    "## 3 Model Explainers <a class=\"anchor\" id=\"ch3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-commander",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=clf.predict(X_test)\n",
    "probabilities = clf.predict_proba(X_test)\n",
    "classes_df = pd.DataFrame(predictions_rf)\n",
    "class_names = classes_df[0].unique()\n",
    "feature_names = X_original.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array([\"Setosa\",\"Versicolor\",\"Virginica\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.where(predictions==\"Iris-setosa\", \"Setosa\", predictions)\n",
    "predictions = np.where(predictions==\"Iris-versicolor\", \"Versicolor\", predictions)\n",
    "predictions = np.where(predictions==\"Iris-virginica\", \"Virginica\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = y_test\n",
    "truth = np.where(truth==\"Iris-setosa\", \"Setosa\", truth)\n",
    "truth = np.where(truth==\"Iris-versicolor\", \"Versicolor\", truth)\n",
    "truth = np.where(truth==\"Iris-virginica\", \"Virginica\", truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-origin",
   "metadata": {},
   "source": [
    "Select which prediction from the test set to explain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_idx = 2 # <- you can change this to select specific prediction to explain\n",
    "class_idx = class_names.tolist().index(predictions[pred_idx])\n",
    "\n",
    "print(\"Predicted class: \", predictions[pred_idx])\n",
    "print(\"True class: \", truth[pred_idx])\n",
    "print(\"\\nPredicted probabilities:\")\n",
    "\n",
    "iter=0\n",
    "for label in class_names:\n",
    "    print(label,\": \",probabilities[pred_idx][iter])\n",
    "    iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-vermont",
   "metadata": {},
   "source": [
    "### 3.1 LIME <a class=\"anchor\" id=\"ch3.1\"></a>\n",
    "https://github.com/marcotcr/lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=feature_names, class_names=class_names, discretize_continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-gasoline",
   "metadata": {},
   "source": [
    "Visualize local explanation of the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-planning",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = lime_explainer.explain_instance(X_test[4], clf.predict_proba, num_features=5, top_labels = 3)\n",
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "explist=exp.as_list()\n",
    "explist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-governor",
   "metadata": {},
   "source": [
    "### 3.2 Anchor Explanations <a class=\"anchor\" id=\"ch3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict_proba(x)\n",
    "anchor_explainer = AnchorTabular(predict_fn, feature_names)\n",
    "anchor_explainer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "anch_exp = anchor_explainer.explain(X_test[pred_idx], threshold=0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_names = target_names\n",
    "\n",
    "anch_exp = anchor_explainer.explain(X_test[pred_idx], threshold=0.95)\n",
    "print('\\nANCHOR:\\n\\nIF %s' % ('\\n AND '.join(anch_exp.anchor)))\n",
    "print('THEN PREDICT: ', class_names[anchor_explainer.predictor(X_test[pred_idx].reshape(1, -1))[0]])\n",
    "print('\\nWITH PRECISION: %.2f' % anch_exp.precision)\n",
    "print('WITH COVERAGE: %.2f' % anch_exp.coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Prediction: ', class_names[anchor_explainer.predictor(X_test[pred_idx].reshape(1, -1))[0]])\n",
    "print('\\nAnchor:\\n %s' % ('\\n AND '.join(anch_exp.anchor)))\n",
    "print('\\nPrecision: %.2f' % anch_exp.precision)\n",
    "print('Coverage: %.2f' % anch_exp.coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sepal Length (cm): \", X_test[pred_idx][0])\n",
    "print(\"Sepal Width (cm): \", X_test[pred_idx][1])\n",
    "print(\"Petal Length (cm): \", X_test[pred_idx][2])\n",
    "print(\"Petal Width (cm): \", X_test[pred_idx][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-desire",
   "metadata": {},
   "source": [
    "### 3.3 Counterfactuals Guided by Prototypes <a class=\"anchor\" id=\"ch3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test[pred_idx].reshape((1,) + X_test[12].shape)\n",
    "shape = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize explainer, fit and generate counterfactual\n",
    "cf = CounterFactualProto(predict_fn, shape, use_kdtree=True, theta=10., max_iterations=500,\n",
    "                         feature_range=(X_train.min(axis=0), X_train.max(axis=0)), \n",
    "                         c_init=1., c_steps=10, eps=(0.05, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-mortality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf.fit(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-hazard",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "startTimeModule = datetime.now()\n",
    "explanation = cf.explain(X)\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'Original prediction: {class_names[explanation.orig_class]}')\n",
    "print('Nearest counterfactual instance: {}'.format(class_names[explanation.cf['class']]))\n",
    "print('Probabilities: ',round(explanation.cf['proba'][0][0],2),\" \",\n",
    "      round(explanation.cf['proba'][0][1],2),\" \",\n",
    "      round(explanation.cf['proba'][0][2],2),)\n",
    "print('\\nSmallest feature value changes necessary:\\n')\n",
    "orig = X\n",
    "counterfactual = explanation.cf['X']\n",
    "delta = counterfactual - orig\n",
    "for i, f in enumerate(feature_names):\n",
    "    if np.abs(delta[0][i]) > 1e-4:\n",
    "        print('{}: {:.2f}  -->   {:.2f}'.format(f,orig[0][i], counterfactual[0][i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "early-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = X\n",
    "counterfactual = explanation.cf['X']\n",
    "delta = counterfactual - orig\n",
    "for i, f in enumerate(feature_names):\n",
    "    if np.abs(delta[0][i]) > 1e-4:\n",
    "        print('{}: {:.2f}  -->   {:.2f}'.format(f,orig[0][i], counterfactual[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-bermuda",
   "metadata": {},
   "source": [
    "### 3.4 Whitebox: Decision Tree <a class=\"anchor\" id=\"ch3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = clf_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dt = np.where(predictions_dt==\"Iris-setosa\", \"Setosa\", predictions_dt)\n",
    "predictions_dt = np.where(predictions_dt==\"Iris-versicolor\", \"Versicolor\", predictions_dt)\n",
    "predictions_dt = np.where(predictions_dt==\"Iris-virginica\", \"Virginica\", predictions_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classes_df = pd.DataFrame(predictions_dt)\n",
    "tree_feature_names = X_original.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf_dt,\n",
    "                     feature_names=tree_feature_names, \n",
    "                     class_names=class_names,\n",
    "                     filled=False, rounded=True,\n",
    "                     special_characters=True,\n",
    "                     out_file=None,)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.format = \"png\"\n",
    "graph.render(\"iris_tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_dt = clf_dt.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_idx = pred_idx # <- you can change this to select specific prediction to explain\n",
    "class_idx = class_names.tolist().index(predictions_dt[pred_idx])\n",
    "\n",
    "print(\"Predicted class: \", predictions_dt[pred_idx])\n",
    "print(\"True class: \", truth[pred_idx])\n",
    "print(\"\\nPredicted probabilities:\")\n",
    "\n",
    "iter=0\n",
    "for label in class_names:\n",
    "    print(label,\": \",probabilities_dt[pred_idx][iter])\n",
    "    iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "times=[]\n",
    "i = 0\n",
    "while i <= 100:\n",
    "    startTimeModule = datetime.now()\n",
    "    clf_dt.predict(X_train[[1]])\n",
    "    endtime=(datetime.now() - startTimeModule)\n",
    "    times.append(endtime.seconds+(endtime.microseconds/1000/1000))\n",
    "    i+=1\n",
    "    \n",
    "print(statistics.mean(times))\n",
    "print(statistics.stdev(times))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
