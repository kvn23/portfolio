{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "separate-amber",
   "metadata": {},
   "source": [
    "# UCI Heart Disease Detection\n",
    "\n",
    "This notebook is used as part of my thesis on XAI, comparing different XAI methods and libraries.\n",
    "<br/>\n",
    "The purpose of the created models is to classify if a patient is either healthy or has a heart disease.\n",
    "<br/>\n",
    "<br/>\n",
    "Attributes:\n",
    "1. age\n",
    "2. sex (1=male, 0=female)\n",
    "3. chest pain type (4 values)\n",
    "4. resting blood pressure\n",
    "5. serum cholesterol in milligrams per deciliter (mg/dl)\n",
    "6. fasting blood sugar > 120 mg/dl\n",
    "7. resting electrocardiographic results (values 0,1,2)\n",
    "8. maximum heart rate achieved\n",
    "9. exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. the slope of the peak exercise ST segment\n",
    "12. number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-sequence",
   "metadata": {},
   "source": [
    "## 1 Set up Environment and Dataset <a class=\"anchor\" id=\"ch1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-swimming",
   "metadata": {},
   "source": [
    "### 1.1 Load Libraries and Set Up Parameters <a class=\"anchor\" id=\"ch1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for reproduction\n",
    "seedNum = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "import catboost\n",
    "import shap\n",
    "import lime\n",
    "import graphviz\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from alibi.explainers import AnchorTabular, CounterFactualProto, CounterFactual\n",
    "from alibi.utils.mapping import ohe_to_ord, ord_to_ohe\n",
    "from datetime import datetime\n",
    "\n",
    "# required installs:\n",
    "# pip install shap\n",
    "# pip install lime\n",
    "# pip install alibi\n",
    "# conda install python-graphviz AND install from https://graphviz.org/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# set up n_jobs\n",
    "n_jobs = 6\n",
    "\n",
    "# set flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.20\n",
    "\n",
    "# set number of folds for cross validation\n",
    "n_folds = 10\n",
    "\n",
    "# set various default modeling parameters\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-shoulder",
   "metadata": {},
   "source": [
    "1. age\n",
    "2. sex\n",
    "3. chest pain type (4 values)\n",
    "4. resting blood pressure\n",
    "5. serum cholesterol in milligrams per deciliter (mg/dl)\n",
    "6. fasting blood sugar > 120 mg/dl\n",
    "7. resting electrocardiographic results (values 0,1,2)\n",
    "8. maximum heart rate achieved\n",
    "9. exercise induced angina\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. the slope of the peak exercise ST segment\n",
    "12. number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order of columns for explanation compatability\n",
    "new_order=[\"sex\",\"cp\",\"fbs\",\"restecg\",\"exang\",\"slope\",\"thal\",\"age\",\"trestbps\",\"chol\",\"thalach\",\"oldpeak\",\"ca\",\"target\"]\n",
    "\n",
    "# dictionary of categorical variable values\n",
    "category_map={0: [\"female\", \"male\"],\n",
    "              1: [\"typical angina\",\"atypical angina\",\"non-anginal pain\",\"asymptomatic\"],\n",
    "              2: [\"below 120 mg/dl\",\"above 120 ml/dl\"],\n",
    "              3: [\"normal\",\"ST-T wave abnormality\",\"probable left ventricular hypertrophy\"],\n",
    "              4: [\"no\",\"yes\"],\n",
    "              5: [\"upsloping\",\"flat\",\"downsloping\"],\n",
    "              6: [\"no info\",\"normal\",\"fixed defect\",\"reversable defect\"]\n",
    "             }\n",
    "\n",
    "# dict of column names for renaming\n",
    "col_names = {\"cp\":'chest pain type', \"trestbps\":'resting blood pressure', \"chol\":'serum cholesterol (mg/dl)',\n",
    "             \"fbs\":'fasting blood sugar', \"restecg\":'resting ecg results',\n",
    "             \"thalach\":'maximum heart rate achieved', \"exang\":'exercise induced angina',\n",
    "             \"oldpeak\":'exercise induced ST depression',\n",
    "             \"slope\":'slope of peak exercise ST segment', \"ca\":'vessels colored by flourosopy', \"thal\":\"thalassemia type\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset_path = 'data/heart.csv'\n",
    "Xy_original = pd.read_csv(dataset_path)\n",
    "Xy_original = Xy_original[new_order]\n",
    "Xy_original.rename(columns=col_names, inplace=True)\n",
    "Xy_original.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-syndrome",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing <a class=\"anchor\" id=\"ch1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-roulette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(Xy_original.columns)\n",
    "totAttr = totCol-1\n",
    "\n",
    "X_original = Xy_original.iloc[:,0:totAttr]\n",
    "y_original = Xy_original.iloc[:,totAttr]\n",
    "\n",
    "print(\"Xy_original.shape: {} X_original.shape: {} y_original.shape: {}\".format(Xy_original.shape, X_original.shape, y_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-emission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with the number of categories for each variable in the dataset\n",
    "cat_vars_ord = {}\n",
    "n_categories = len(list(category_map.keys()))\n",
    "for i in range(n_categories):\n",
    "    cat_vars_ord[i] = len(np.unique(X_original.to_numpy()[:, i]))\n",
    "print(cat_vars_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars_ohe = ord_to_ohe(X_original.to_numpy(), cat_vars_ord)[1]\n",
    "print(cat_vars_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X_original.to_numpy()[:, -6:].astype(np.float32, copy=False)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_num_scaled= scaler.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X_original.to_numpy()[:, :-6].copy()\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "ohe.fit(X_cat)\n",
    "X_cat_ohe = ohe.transform(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc = np.c_[X_cat_ohe.todense(), X_num_scaled].astype(np.float32, copy=False)\n",
    "\n",
    "X_enc = pd.DataFrame(X_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data further into training and test datasets\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_enc, y_original, test_size=splitPercentage, \n",
    "                                                                stratify=y_original, random_state=seedNum)\n",
    "\n",
    "print(\"X_train.shape: {} y_train_df.shape: {}\".format(X_train_df.shape, y_train_df.shape))\n",
    "print(\"X_test_df.shape: {} y_test_df.shape: {}\".format(X_test_df.shape, y_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize the training and testing datasets for the modeling activities\n",
    "X_train = X_train_df.to_numpy()\n",
    "y_train = y_train_df.to_numpy()\n",
    "X_test = X_test_df.to_numpy()\n",
    "y_test = y_test_df.to_numpy()\n",
    "print(\"X_train.shape: {} y_train.shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"X_test.shape: {} y_test.shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-engagement",
   "metadata": {},
   "source": [
    "## 2 Tree-based Modeling <a class=\"anchor\" id=\"ch2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-cemetery",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "tune_model = RandomForestClassifier(random_state=seedNum, n_jobs=n_jobs)\n",
    "\n",
    "n_estimators = [100]\n",
    "criterion = [\"gini\",\"entropy\"]\n",
    "max_features =[None, \"sqrt\", 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "paramGrid = dict(n_estimators=n_estimators, criterion=criterion, max_features=max_features)\n",
    "\n",
    "kfold = KFold(n_splits=n_folds)\n",
    "grid = GridSearchCV(estimator=tune_model, param_grid=paramGrid, scoring=scoring, cv=kfold, refit=\"Recall\")\n",
    "grid_result = grid.fit(X_enc, y_original)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_rf_be = grid_result.best_estimator_\n",
    "clf_rf = clf_rf_be.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-catholic",
   "metadata": {},
   "source": [
    "Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cb_be = CatBoostClassifier(eval_metric='Accuracy', depth=6, verbose=False)\n",
    "clf_cb = clf_cb_be.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-bracelet",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_rf = clf_rf.predict(X_test)\n",
    "predictions_cb = clf_cb.predict(X_test)\n",
    "cv_rf = cross_val_score(clf_rf_be, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "cv_cb = cross_val_score(clf_cb_be, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(clf_rf,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_rf))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_rf))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_rf.mean(), cv_rf.std()))\n",
    "print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "print(clf_cb,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_cb))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_cb))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_cb.mean(), cv_cb.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-finnish",
   "metadata": {},
   "source": [
    "## 3 Counterfactuals <a class=\"anchor\" id=\"ch3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=clf_cb\n",
    "pred_idx = 60\n",
    "\n",
    "probabilities = clf.predict_proba(X_test)\n",
    "print(\"Probabilities: \", probabilities[pred_idx])\n",
    "print(\"Correct class: \", y_test[pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"healthy\", \"sick\"]\n",
    "feature_names = X_original.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-sarah",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = X_test[pred_idx].reshape((1,) + X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[pred_idx].reshape((1,) + X_test[0].shape)\n",
    "\n",
    "shape = x.shape\n",
    "beta = .01\n",
    "c_init = 1.\n",
    "c_steps = 5\n",
    "max_iterations = 500\n",
    "rng = (-1., 1.)  # scale features between -1 and 1\n",
    "rng_shape = (1,) + X_original.shape[1:]\n",
    "feature_range = ((np.ones(rng_shape) * rng[0]).astype(np.float32),\n",
    "                 (np.ones(rng_shape) * rng[1]).astype(np.float32))\n",
    "use_kdtree = True\n",
    "theta = 10.  # weight of prototype loss term\n",
    "\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "cf = CounterFactualProto(predict_fn,\n",
    "                         shape,\n",
    "                         beta=beta,\n",
    "                         theta=theta,\n",
    "                         cat_vars=cat_vars_ohe,\n",
    "                         ohe=True,\n",
    "                         use_kdtree=use_kdtree,\n",
    "                         max_iterations=max_iterations,\n",
    "                         feature_range=feature_range,\n",
    "                         c_init=c_init,\n",
    "                         c_steps=c_steps,\n",
    "                         eps=(0.05, 0.05)\n",
    "                        )\n",
    "\n",
    "cf.fit(X_train, d_type='abdm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTimeModule = datetime.now()\n",
    "explanation = cf.explain(x)\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_instance(X, explanation, eps=1e-2):\n",
    "    print('Prediction by the model: {}  -- proba: {}'.format(target_names[explanation.orig_class],\n",
    "                                                       explanation.orig_proba[0]))\n",
    "    print('Counterfactual instance: {}  -- proba: {}'.format(target_names[explanation.cf['class']],\n",
    "                                                             explanation.cf['proba'][0]))\n",
    "    print('\\nCounterfactual perturbations...')\n",
    "    \n",
    "    print('\\nCategorical:')\n",
    "    X_orig_ord = ohe_to_ord(X, cat_vars_ohe)[0]\n",
    "    X_cf_ord = ohe_to_ord(explanation.cf['X'], cat_vars_ohe)[0]\n",
    "    delta_cat = {}\n",
    "    for i, (_, v) in enumerate(category_map.items()):\n",
    "        cat_orig = v[int(X_orig_ord[0, i])]\n",
    "        cat_cf = v[int(X_cf_ord[0, i])]\n",
    "        if cat_orig != cat_cf:\n",
    "            delta_cat[feature_names[i]] = [cat_orig, cat_cf]\n",
    "    if delta_cat:\n",
    "        for k, v in delta_cat.items():\n",
    "            print('{}: {}  -->   {}'.format(k, v[0], v[1]))\n",
    "    \n",
    "    print('\\nNumerical:')\n",
    "    delta_num = X_cf_ord[0, -6:] - X_orig_ord[0, -6:]\n",
    "    n_keys = len(list(cat_vars_ord.keys()))\n",
    "    X_orig_num = scaler.inverse_transform(X_orig_ord[0,-6:].reshape(1,-1))\n",
    "    X_cf_num = scaler.inverse_transform(X_cf_ord[0,-6:].reshape(1,-1))\n",
    "    for i in range(delta_num.shape[0]):\n",
    "        if np.abs(delta_num[i]) > eps:\n",
    "            print('{}: {:.2f}  -->   {:.2f}'.format(feature_names[i+n_keys],\n",
    "                                            #X_orig_ord[0,i+n_keys],\n",
    "                                            X_orig_num[0,i],\n",
    "                                            #X_cf_ord[0,i+n_keys]))\n",
    "                                            X_cf_num[0,i]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_instance2(X, explanation, eps=1e-2):\n",
    "    print('Nearest counterfactual instance: {}'.format(target_names[explanation.cf['class']]))\n",
    "    print('Probabilities: ',round(explanation.cf['proba'][0][0],2),\" \",round(explanation.cf['proba'][0][1],2))\n",
    "    \n",
    "    print('\\nSmallest feature value changes necessary:\\n')\n",
    "    \n",
    "    #print('\\nCategorical:')\n",
    "    X_orig_ord = ohe_to_ord(X, cat_vars_ohe)[0]\n",
    "    X_cf_ord = ohe_to_ord(explanation.cf['X'], cat_vars_ohe)[0]\n",
    "    delta_cat = {}\n",
    "    for i, (_, v) in enumerate(category_map.items()):\n",
    "        cat_orig = v[int(X_orig_ord[0, i])]\n",
    "        cat_cf = v[int(X_cf_ord[0, i])]\n",
    "        if cat_orig != cat_cf:\n",
    "            delta_cat[feature_names[i]] = [cat_orig, cat_cf]\n",
    "    if delta_cat:\n",
    "        for k, v in delta_cat.items():\n",
    "            print('{}: {}  -->   {}'.format(k, v[0], v[1]))\n",
    "    \n",
    "    #print('\\nNumerical:')\n",
    "    delta_num = X_cf_ord[0, -6:] - X_orig_ord[0, -6:]\n",
    "    n_keys = len(list(cat_vars_ord.keys()))\n",
    "    X_orig_num = scaler.inverse_transform(X_orig_ord[0,-6:].reshape(1,-1))\n",
    "    X_cf_num = scaler.inverse_transform(X_cf_ord[0,-6:].reshape(1,-1))\n",
    "    for i in range(delta_num.shape[0]):\n",
    "        if np.abs(delta_num[i]) > eps:\n",
    "            print('{}: {:.2f}  -->   {:.2f}'.format(feature_names[i+n_keys],\n",
    "                                            #X_orig_ord[0,i+n_keys],\n",
    "                                            X_orig_num[0,i],\n",
    "                                            #X_cf_ord[0,i+n_keys]))\n",
    "                                            X_cf_num[0,i]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-gilbert",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "describe_instance2(x, explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-animal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
