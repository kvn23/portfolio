{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increased-dressing",
   "metadata": {},
   "source": [
    "# German Credit Classification\n",
    "\n",
    "This notebook is used as part of my thesis, comparing different XAI methods and libraries.\n",
    "<br/>\n",
    "The purpose of the created models is to classify the risk of credit applicants to decide if the credit should be denied.\n",
    "<br/>\n",
    "The result is then explained with the help of [Counterfactual Instances](https://docs.seldon.io/projects/alibi/en/stable/methods/CF.html).\n",
    "<br/>\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-rescue",
   "metadata": {},
   "source": [
    "## 1 Set up Environment and Dataset <a class=\"anchor\" id=\"ch1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-smoke",
   "metadata": {},
   "source": [
    "### 1.1 Load Libraries and Set Up Parameters <a class=\"anchor\" id=\"ch1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blank-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for reproduction\n",
    "seedNum = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "genetic-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import urllib.request\n",
    "import seaborn as sns\n",
    "import catboost\n",
    "import shap\n",
    "import lime\n",
    "import graphviz\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.inspection import partial_dependence, plot_partial_dependence\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from alibi.explainers import AnchorTabular, CounterFactualProto, CounterFactual\n",
    "from alibi.utils.mapping import ohe_to_ord, ord_to_ohe\n",
    "from datetime import datetime\n",
    "\n",
    "# required installs:\n",
    "# pip install shap\n",
    "# pip install lime\n",
    "# pip install alibi\n",
    "# conda install python-graphviz AND install from https://graphviz.org/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brown-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# set up n_jobs\n",
    "n_jobs = 6\n",
    "\n",
    "# set flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.20\n",
    "\n",
    "# set number of folds for cross validation\n",
    "n_folds = 10\n",
    "\n",
    "# set various default modeling parameters\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cooked-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column names\n",
    "col_names = ['existing checking', 'credit duration (months)', 'credit history', 'credit purpose', 'credit amount',\n",
    "             'existing savings', 'employment since', 'installment rate', 'sex and marital status', 'other debtors',\n",
    "             'residence since (years)', 'property', 'age', 'other installment plans', 'housing',\n",
    "             'existing credits', 'job', 'people liable', 'telephone', 'foreign worker', 'target']\n",
    "\n",
    "# reordered list of column names, with all categorical variables at the front\n",
    "new_order = ['existing checking', 'credit history', 'credit purpose', 'existing savings', 'employment since',\n",
    "             'sex and marital status', 'other debtors', 'property', 'other installment plans', 'housing', 'job',\n",
    "             'telephone', 'foreign worker', 'credit duration (months)', 'credit amount', 'installment rate',\n",
    "             'residence since (years)','age','existing credits', 'people liable', 'target']\n",
    "\n",
    "# dictionary of original categorical variable values and their encoded value\n",
    "replace_with = {\"A11\" : 0, \"A12\" : 1, \"A13\" : 2, \"A14\" : 3,\n",
    "                \"A30\" : 0, \"A31\" : 1, \"A32\" : 2, \"A33\" : 3, \"A34\" : 4,\n",
    "                \"A40\" : 0, \"A41\" : 1, \"A42\" : 2, \"A43\" : 3, \"A44\" : 4, \"A45\" : 5,\n",
    "                \"A46\" : 6, \"A48\" : 7, \"A49\" : 8, \"A410\" : 9,\n",
    "                \"A61\" : 0, \"A62\" : 1, \"A63\" : 2, \"A64\" : 3, \"A65\" : 4,\n",
    "                \"A71\" : 0, \"A72\" : 1, \"A73\" : 2, \"A74\" : 3, \"A75\" : 4,\n",
    "                \"A91\" : 0, \"A92\" : 1, \"A93\" : 2, \"A94\" : 3,\n",
    "                \"A101\" : 0, \"A102\" : 1, \"A103\" : 2,\n",
    "                \"A121\" : 0, \"A122\" : 1, \"A123\" : 2, \"A124\" : 3,\n",
    "                \"A141\" : 0, \"A142\" : 1, \"A143\" : 2,\n",
    "                \"A151\" : 0, \"A152\" : 1, \"A153\" : 2,\n",
    "                \"A171\" : 0, \"A172\" : 1, \"A173\" : 2, \"A174\" : 3,\n",
    "                \"A191\" : 0, \"A192\" : 1,\"A201\" : 0, \"A202\" : 1\n",
    "               }\n",
    "\n",
    "# dictionary of categorical variable values\n",
    "category_map={0: [\"0 or less\", \"0 to 200\", \"more than 200\", \"no checking account\"],\n",
    "              1: [\"no credits taken / all paid back duly\", \"all credits at this bank paid back duly\",\n",
    "                  \"all paid back duly until now\", \"delay in paying off in the past\",\n",
    "                  \"critical account/ other credits existing (not at this bank)\"],\n",
    "              2: [\"car (new)\", \"car (used)\", \"furniture/equipment\", \"radio/television\",\n",
    "                  \"domestic appliances\", \"repairs\", \"education\", \"retraining\", \"business\", \"others\"],\n",
    "              3: [\"less than 100\", \"100 to 500\", \"500 to 1000\", \"more than 1000\", \"unknown/ no savings account\"],\n",
    "              4: [\"unemployed\", \"less than 1 year\", \"1 to 4 years\", \"4 to 7 years\", \"more than 7 years\"],\n",
    "              5: [\"male : divorced/separated\", \"female : divorced/separated/married\", \"male : single\",\n",
    "                  \"male : married/widowed\"],\n",
    "              6: [\"none\", \"co-applicant\", \"guarantor\"],\n",
    "              7: [\"real estate\", \"life insurance\", \"car or other\", \"unknown / no property\"], \n",
    "              8: [\"bank\", \"stores\", \"none\"], \n",
    "              9: [\"rent\", \"own\", \"for free\"], \n",
    "              10: [\"unemployed/ unskilled - non-resident\", \"unskilled - resident\", \"skilled employee / official\",\n",
    "                   \"management/self-employed/highly qualified/officer\"], \n",
    "              11: [\"no\", \"yes\"], \n",
    "              12: [\"yes\", \"no\"], \n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fresh-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset\n",
    "dataset_path = 'http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'\n",
    "Xy_original = pd.read_csv(dataset_path, names=col_names, sep=' ', header=None)\n",
    "Xy_original = Xy_original[new_order]\n",
    "Xy_original.replace(replace_with, inplace=True)\n",
    "Xy_original.replace({\"target\":{1:0, 2:1}},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "qualified-commander",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    700\n",
      "1    300\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(Xy_original.groupby(\"target\").size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-spokesman",
   "metadata": {},
   "source": [
    "### 1.2 Preprocessing <a class=\"anchor\" id=\"ch1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "listed-halifax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xy_original.shape: (1000, 21) X_original.shape: (1000, 20) y_original.shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(Xy_original.columns)\n",
    "totAttr = totCol-1\n",
    "\n",
    "X_original = Xy_original.iloc[:,0:totAttr]\n",
    "y_original = Xy_original.iloc[:,totAttr]\n",
    "\n",
    "print(\"Xy_original.shape: {} X_original.shape: {} y_original.shape: {}\".format(Xy_original.shape, X_original.shape, y_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "biblical-camcorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 1: 5, 2: 10, 3: 5, 4: 5, 5: 4, 6: 3, 7: 4, 8: 3, 9: 3, 10: 4, 11: 2, 12: 2}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary with the number of categories for each variable in the dataset\n",
    "cat_vars_ord = {}\n",
    "n_categories = len(list(category_map.keys()))\n",
    "for i in range(n_categories):\n",
    "    cat_vars_ord[i] = len(np.unique(X_original.to_numpy()[:, i]))\n",
    "print(cat_vars_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prime-steps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 4, 4: 5, 9: 10, 19: 5, 24: 5, 29: 4, 33: 3, 36: 4, 40: 3, 43: 3, 46: 4, 50: 2, 52: 2}\n"
     ]
    }
   ],
   "source": [
    "# create dictionary containing the first column index for each one-hot encoded categorical variable\n",
    "cat_vars_ohe = ord_to_ohe(X_original.to_numpy(), cat_vars_ord)[1]\n",
    "print(cat_vars_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "biblical-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = X_original.to_numpy()[:, -7:].astype(np.float32, copy=False)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_num_scaled= scaler.fit_transform(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collected-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = X_original.to_numpy()[:, :13].copy()\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "ohe.fit(X_cat)\n",
    "X_cat_ohe = ohe.transform(X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "institutional-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_enc = np.c_[X_cat_ohe.todense(), X_num_scaled].astype(np.float32, copy=False)\n",
    "\n",
    "X_enc = pd.DataFrame(X_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "above-batch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (800, 61) y_train_df.shape: (800,)\n",
      "X_test_df.shape: (200, 61) y_test_df.shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data further into training and test datasets\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X_enc, y_original, test_size=splitPercentage, \n",
    "                                                                stratify=y_original, random_state=seedNum)\n",
    "\n",
    "print(\"X_train.shape: {} y_train_df.shape: {}\".format(X_train_df.shape, y_train_df.shape))\n",
    "print(\"X_test_df.shape: {} y_test_df.shape: {}\".format(X_test_df.shape, y_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wrong-institution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (800, 61) y_train.shape: (800,)\n",
      "X_test.shape: (200, 61) y_test.shape: (200,)\n"
     ]
    }
   ],
   "source": [
    "# Finalize the training and testing datasets for the modeling activities\n",
    "X_train = X_train_df.to_numpy()\n",
    "y_train = y_train_df.to_numpy()\n",
    "X_test = X_test_df.to_numpy()\n",
    "y_test = y_test_df.to_numpy()\n",
    "print(\"X_train.shape: {} y_train.shape: {}\".format(X_train.shape, y_train.shape))\n",
    "print(\"X_test.shape: {} y_test.shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-watson",
   "metadata": {},
   "source": [
    "## 2 Tree-based Modeling <a class=\"anchor\" id=\"ch2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-spelling",
   "metadata": {},
   "source": [
    "Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "biological-doctor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.760000 using {'criterion': 'gini', 'max_features': 0.4, 'n_estimators': 100}\n",
      "Computing time: 0:00:12.219097\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "tune_model = RandomForestClassifier(random_state=seedNum, n_jobs=n_jobs)\n",
    "\n",
    "n_estimators = [100]\n",
    "criterion = [\"gini\",\"entropy\"]\n",
    "max_features =[None, \"sqrt\", 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "paramGrid = dict(n_estimators=n_estimators, criterion=criterion, max_features=max_features)\n",
    "\n",
    "kfold = KFold(n_splits=n_folds)\n",
    "grid = GridSearchCV(estimator=tune_model, param_grid=paramGrid, scoring=scoring, cv=kfold, refit=\"Accuracy\")\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))\n",
    "\n",
    "clf_rf_be = grid_result.best_estimator_\n",
    "clf_rf = clf_rf_be.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-roads",
   "metadata": {},
   "source": [
    "Gradient Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "egyptian-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_cb_be = CatBoostClassifier(eval_metric='Accuracy', depth=6, verbose=False)\n",
    "clf_cb = clf_cb_be.fit(X_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-navigator",
   "metadata": {},
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subtle-segment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_features=0.4, n_jobs=6, random_state=23) \n",
      "Confusion Matrix:\n",
      "[[127  13]\n",
      " [ 33  27]]\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       140\n",
      "           1       0.68      0.45      0.54        60\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.73      0.68      0.69       200\n",
      "weighted avg       0.76      0.77      0.75       200\n",
      "\n",
      "Cross-Validation: 0.760000 (0.024238)\n",
      "--------------------------------------------------------\n",
      "\n",
      "<catboost.core.CatBoostClassifier object at 0x0000020F8A816040> \n",
      "Confusion Matrix:\n",
      "[[126  14]\n",
      " [ 27  33]]\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       140\n",
      "           1       0.70      0.55      0.62        60\n",
      "\n",
      "    accuracy                           0.80       200\n",
      "   macro avg       0.76      0.73      0.74       200\n",
      "weighted avg       0.79      0.80      0.79       200\n",
      "\n",
      "Cross-Validation: 0.771250 (0.030644)\n"
     ]
    }
   ],
   "source": [
    "predictions_rf = clf_rf.predict(X_test)\n",
    "predictions_cb = clf_cb.predict(X_test)\n",
    "cv_rf = cross_val_score(clf_rf_be, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "cv_cb = cross_val_score(clf_cb_be, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "\n",
    "print(clf_rf,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_rf))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_rf))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_rf.mean(), cv_rf.std()))\n",
    "print(\"--------------------------------------------------------\\n\")\n",
    "\n",
    "print(clf_cb,\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, predictions_cb))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\",classification_report(y_test, predictions_cb))\n",
    "print(\"Cross-Validation: %f (%f)\" % (cv_cb.mean(), cv_cb.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-grave",
   "metadata": {},
   "source": [
    "### 3 Counterfactuals <a class=\"anchor\" id=\"ch3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "green-excuse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities:  [0.91281177 0.08718823]\n",
      "Correct class:  0\n"
     ]
    }
   ],
   "source": [
    "clf=clf_cb\n",
    "pred_idx = 3\n",
    "\n",
    "probabilities = clf.predict_proba(X_test)\n",
    "print(\"Probabilities: \", probabilities[pred_idx])\n",
    "print(\"Correct class: \", y_test[pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "piano-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [\"good credit\", \"bad credit\"]\n",
    "feature_names = X_original.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "quality-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test[pred_idx].reshape((1,) + X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "occupied-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_fn = lambda x: clf.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "wooden-protocol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\k23\\anaconda3\\envs\\ensemblesXAI\\lib\\site-packages\\alibi\\utils\\tf.py:26: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = X_test[pred_idx].reshape((1,) + X_test[0].shape)\n",
    "\n",
    "shape = x.shape\n",
    "beta = .01\n",
    "c_init = 1.\n",
    "c_steps = 5\n",
    "max_iterations = 500\n",
    "rng = (-1., 1.)  # scale features between -1 and 1\n",
    "rng_shape = (1,) + X_original.shape[1:]\n",
    "feature_range = ((np.ones(rng_shape) * rng[0]).astype(np.float32),\n",
    "                 (np.ones(rng_shape) * rng[1]).astype(np.float32))\n",
    "use_kdtree = True\n",
    "theta = 10.  # weight of prototype loss term\n",
    "\n",
    "\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "cf = CounterFactualProto(predict_fn,\n",
    "                         shape,\n",
    "                         beta=beta,\n",
    "                         theta=theta,\n",
    "                         cat_vars=cat_vars_ohe,\n",
    "                         ohe=True,\n",
    "                         use_kdtree=use_kdtree,\n",
    "                         max_iterations=max_iterations,\n",
    "                         feature_range=feature_range,\n",
    "                         c_init=c_init,\n",
    "                         c_steps=c_steps,\n",
    "                         eps=(0.05, 0.05)\n",
    "                        )\n",
    "\n",
    "cf.fit(X_train, d_type='abdm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "assigned-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_instance(X, explanation, eps=1e-2):\n",
    "    print('Prediction by the model: {}  -- proba: {}'.format(target_names[explanation.orig_class],\n",
    "                                                       explanation.orig_proba[0]))\n",
    "    print('Counterfactual instance: {}  -- proba: {}'.format(target_names[explanation.cf['class']],\n",
    "                                                             explanation.cf['proba'][0]))\n",
    "    print('\\nCounterfactual perturbations...')\n",
    "    \n",
    "    print('\\nCategorical:')\n",
    "    X_orig_ord = ohe_to_ord(X, cat_vars_ohe)[0]\n",
    "    X_cf_ord = ohe_to_ord(explanation.cf['X'], cat_vars_ohe)[0]\n",
    "    delta_cat = {}\n",
    "    for i, (_, v) in enumerate(category_map.items()):\n",
    "        cat_orig = v[int(X_orig_ord[0, i])]\n",
    "        cat_cf = v[int(X_cf_ord[0, i])]\n",
    "        if cat_orig != cat_cf:\n",
    "            delta_cat[feature_names[i]] = [cat_orig, cat_cf]\n",
    "    if delta_cat:\n",
    "        for k, v in delta_cat.items():\n",
    "            print('{}: {}  -->   {}'.format(k, v[0], v[1]))\n",
    "    \n",
    "    print('\\nNumerical:')\n",
    "    delta_num = X_cf_ord[0, -7:] - X_orig_ord[0, -7:]\n",
    "    n_keys = len(list(cat_vars_ord.keys()))\n",
    "    X_orig_num = scaler.inverse_transform(X_orig_ord[0,-7:].reshape(1,-1))\n",
    "    X_cf_num = scaler.inverse_transform(X_cf_ord[0,-7:].reshape(1,-1))\n",
    "    for i in range(delta_num.shape[0]):\n",
    "        if np.abs(delta_num[i]) > eps:\n",
    "            print('{}: {:.2f}  -->   {:.2f}'.format(feature_names[i+n_keys],\n",
    "                                            #X_orig_ord[0,i+n_keys],\n",
    "                                            X_orig_num[0,i],\n",
    "                                            #X_cf_ord[0,i+n_keys]))\n",
    "                                            X_cf_num[0,i]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "conservative-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_instance2(X, explanation, eps=1e-2):\n",
    "    print('Nearest counterfactual instance: {}'.format(target_names[explanation.cf['class']]))\n",
    "    print('Probabilities: ',round(explanation.cf['proba'][0][0],2),\" \",round(explanation.cf['proba'][0][1],2))\n",
    "    \n",
    "    print('\\nSmallest feature value changes necessary:\\n')\n",
    "    \n",
    "    #print('\\nCategorical:')\n",
    "    X_orig_ord = ohe_to_ord(X, cat_vars_ohe)[0]\n",
    "    X_cf_ord = ohe_to_ord(explanation.cf['X'], cat_vars_ohe)[0]\n",
    "    delta_cat = {}\n",
    "    for i, (_, v) in enumerate(category_map.items()):\n",
    "        cat_orig = v[int(X_orig_ord[0, i])]\n",
    "        cat_cf = v[int(X_cf_ord[0, i])]\n",
    "        if cat_orig != cat_cf:\n",
    "            delta_cat[feature_names[i]] = [cat_orig, cat_cf]\n",
    "    if delta_cat:\n",
    "        for k, v in delta_cat.items():\n",
    "            print('{}: {}  -->   {}'.format(k, v[0], v[1]))\n",
    "    \n",
    "    #print('\\nNumerical:')\n",
    "    delta_num = X_cf_ord[0, -6:] - X_orig_ord[0, -6:]\n",
    "    n_keys = len(list(cat_vars_ord.keys()))\n",
    "    X_orig_num = scaler.inverse_transform(X_orig_ord[0,-6:].reshape(1,-1))\n",
    "    X_cf_num = scaler.inverse_transform(X_cf_ord[0,-6:].reshape(1,-1))\n",
    "    for i in range(delta_num.shape[0]):\n",
    "        if np.abs(delta_num[i]) > eps:\n",
    "            print('{}: {:.2f}  -->   {:.2f}'.format(feature_names[i+n_keys],\n",
    "                                            #X_orig_ord[0,i+n_keys],\n",
    "                                            X_orig_num[0,i],\n",
    "                                            #X_cf_ord[0,i+n_keys]))\n",
    "                                            X_cf_num[0,i]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "behavioral-angel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing time: 0:00:22.613746\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "explanation = cf.explain(x)\n",
    "print ('Computing time:',(datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "detected-burning",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest counterfactual instance: bad credit\n",
      "Probabilities:  0.31   0.69\n",
      "\n",
      "Smallest feature value changes necessary:\n",
      "\n",
      "existing checking: no checking account  -->   0 or less\n",
      "employment since: more than 7 years  -->   1 to 4 years\n",
      "property: life insurance  -->   car or other\n",
      "other installment plans: none  -->   bank\n",
      "housing: own  -->   rent\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,6) (7,) (1,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ef0187df589c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdescribe_instance2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplanation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-250ebd754923>\u001b[0m in \u001b[0;36mdescribe_instance2\u001b[1;34m(X, explanation, eps)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mdelta_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_cf_ord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX_orig_ord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mn_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_vars_ord\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mX_orig_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_orig_ord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mX_cf_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_cf_ord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_num\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    459\u001b[0m                         force_all_finite=\"allow-nan\")\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,6) (7,) (1,6) "
     ]
    }
   ],
   "source": [
    "describe_instance2(x, explanation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
